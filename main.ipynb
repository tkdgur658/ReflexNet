{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b271229-3629-424a-9e5c-9dcc531af0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import sys\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from monai.losses import TverskyLoss as TverskyLoss\n",
    "\n",
    "from metrics import *\n",
    "from data_loader import *\n",
    "from utils import *\n",
    "from train import *\n",
    "\n",
    "model_name = 'ReflexNet'\n",
    "exec(f'from {model_name} import *')\n",
    "\n",
    "Dataset_dir = '../Total_Datasets/Reflex_Dataset'\n",
    "\n",
    "iterations = [1, 30]\n",
    "train_size=0.6\n",
    "\n",
    "in_channels = 3\n",
    "number_of_classes=1\n",
    "epochs = 50\n",
    "batch_size = 8\n",
    "\n",
    "devices = [0,1]\n",
    "\n",
    "optimizer = 'AdamW'\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "optim_args = {'optimizer': optimizer, 'lr': lr, 'momentum': momentum, 'weight_decay': weight_decay}\n",
    "\n",
    "lr_scheduler = 'CosineAnnealingLR'\n",
    "T_max = epochs\n",
    "T_0 = 50\n",
    "eta_min = 1e-6\n",
    "lr_scheduler_args = {'lr_scheduler': lr_scheduler, 'T_max': T_max, 'T_0': T_0, 'eta_min': eta_min}\n",
    "\n",
    "loss_function = 'Tversky Focal Loss'\n",
    "reduction = 'mean'\n",
    "gamma = 2.0\n",
    "weight = None\n",
    "loss_function_args = {'loss_function': loss_function, 'reduction': reduction, 'gamma': gamma, 'weight': weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1278594-47f4-418a-8343-d87000e7b1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contduct_experiments(iteration, model_name, model, train_loader, validation_loader, test_loader, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device, transform):\n",
    "    start = timeit.default_timer()\n",
    "    train_bool=True\n",
    "    test_bool=True\n",
    "    if loss_function == 'Tversky Focal Loss':\n",
    "        criterion=TverskyLoss()\n",
    "    if Optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif Optimizer == 'SGD':\n",
    "        momentum = 0.9\n",
    "        weight_decay = 1e-4\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum ,weight_decay=weight_decay)\n",
    "    elif Optimizer =='AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    if lr_scheduler_args['lr_scheduler'] == 'CosineAnnealingLR':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = lr_scheduler_args['T_max'], eta_min = lr_scheduler_args['eta_min'])\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    control_random_seed(seed)\n",
    "    if train_bool:\n",
    "        now = datetime.now()\n",
    "        Train_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Training Start Time:',Train_date)\n",
    "        best=9999\n",
    "        best_epoch=1\n",
    "        Early_Stop=0\n",
    "        loss_saver = LossSaver()\n",
    "        train_start_time = timeit.default_timer()\n",
    "        for epoch in range(1, epochs+1):\n",
    "            Train_Loss = train(train_loader, epoch, \n",
    "              model, criterion, optimizer, device\n",
    "              )\n",
    "            lr_scheduler.step()\n",
    "            outputs, targets  \\\n",
    "            = validate(validation_loader, \n",
    "              model, criterion, device\n",
    "              )\n",
    "            Val_Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)            \n",
    "            iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "            dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "            now = datetime.now()\n",
    "            date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print(str(epoch)+'EP('+date+'):',end=' ')\n",
    "            print('T_Loss: ' + str(Train_Loss), end=' ')\n",
    "            print('V_Loss: ' + str(Val_Loss), end=' ')\n",
    "            print('IoU: ' + str(iou), end=' ')\n",
    "            print('Dice: ' + str(dice), end=' ')\n",
    "            \n",
    "            loss_saver.update(Train_Loss, Val_Loss)\n",
    "            loss_saver.save_as_csv(f'{output_dir}/Losses_{Experiments_Time}.csv')\n",
    "            if Val_Loss<best:\n",
    "                Early_Stop = 0\n",
    "                torch.save(model.state_dict(), f'{output_dir}/{Train_date}_{model_name}_Iter_{iteration}.pt')\n",
    "                best_epoch = epoch\n",
    "                best = Val_Loss\n",
    "                print('Best Epoch:',best_epoch,'Loss:',Val_Loss)\n",
    "            else:\n",
    "                print('')\n",
    "                Early_Stop+=1\n",
    "            if Early_Stop>=15:\n",
    "                break\n",
    "        train_stop_time = timeit.default_timer()\n",
    "    if test_bool:\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Test Start Time:',date)\n",
    "        outputs, targets, image_paths \\\n",
    "            = validate(test_loader, \n",
    "              model, criterion, device,\n",
    "            model_path=f'{output_dir}/{Train_date}_{model_name}_Iter_{iteration}.pt',\n",
    "                       return_image_paths=True\n",
    "              )        \n",
    "        Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)\n",
    "        iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "        dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "        recall, precision, f1 = Confusion_Matrix(outputs, targets) \n",
    "        recall = np.round(recall.cpu().numpy()[0],3); precision = np.round(precision.cpu().numpy()[0],3); f1 = np.round(f1.cpu().numpy()[0],3);\n",
    "                \n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Best Epoch:',best_epoch)\n",
    "        print('Test('+date+'): '+'Loss: ' + str(Loss),end=' ')\n",
    "        print('IoU: ' + str(iou), end=' ')\n",
    "        print('Dice: ' + str(dice), end=' ')\n",
    "        print('Recall: ' + str(recall), end=' ')\n",
    "        print('Precision: ' + str(precision), end=' ')\n",
    "        print('F1 Score: ' + str(f1), end='\\n')\n",
    "                            \n",
    "        stop = timeit.default_timer();m, s = divmod((train_stop_time - train_start_time)/epoch, 60);h, m = divmod(m, 60);Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s);\n",
    "        m, s = divmod(stop - start, 60);h, m = divmod(m, 60);Time = \"%02d:%02d:%02d\" % (h, m, s);\n",
    "        total_params = sum(p.numel() for p in model.parameters()); total_params = format(total_params , ',');\n",
    "        Performances = [Experiments_Time, Train_date, iteration, model_name, best, Loss, iou, dice, recall, precision, f1, total_params,Time, best_epoch, Time_per_Epoch, loss_function, lr, batch_size, epochs]\n",
    "        df = df.append(pd.Series(Performances, index=df.columns), ignore_index=True)\n",
    "        os.makedirs(f'{output_dir}/test_outputs', exist_ok = True)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        for output, image_path in zip(outputs, image_paths):\n",
    "            np.save(f'{output_dir}/test_outputs/{os.path.basename(image_path)}', output)\n",
    "    now = datetime.now()\n",
    "    date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End',date)\n",
    "    return df\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691908be-458c-416b-8edd-1e07c042414d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Start Time: 240121_032349\n",
      "ReflexNet (Iter 1)\n",
      "Training Start Time: 240121_032350\n",
      "1EP(240121_032417): T_Loss: 0.839045 V_Loss: 0.999631 IoU: 0.0 Dice: 0.0 Best Epoch: 1 Loss: 0.999631\n",
      "Test Start Time: 240121_032417\n",
      "Best Epoch: 1\n",
      "Test(240121_032419): Loss: 0.999592 IoU: 0.0 Dice: 0.0 Recall: 0.0 Precision: 0.0 F1 Score: 0.0\n",
      "End 240121_032419\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "Experiments_Time=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:',Experiments_Time)\n",
    "Metrics=['Experiment Time','Train Time', 'Iteration','Model Name', 'Val_Loss', 'Test_Loss', 'IoU', 'Dice', 'Recall', 'Precision', 'F1 Score', 'Total Params','Train-Predction Time','Best Epoch','Time per Epoch', 'Loss Function', 'LR', 'Batch size', '#Epochs']\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "output_root = f'output/output_{Experiments_Time}'\n",
    "os.makedirs(output_root, exist_ok = True)\n",
    "    \n",
    "for iteration in range(iterations[0], iterations[1]+1):\n",
    "    seed = iteration\n",
    "    image_path_list=natsort.natsorted(glob.glob(Dataset_dir+'/Originals/*')); target_path_list=[]\n",
    "    for image_path in image_path_list:\n",
    "        target_path_list.append(Dataset_dir+'/Masks/'+os.path.basename(image_path))\n",
    "    num_dataset = len(target_path_list)\n",
    "    indices = list(range(num_dataset))\n",
    "    split1=int(train_size*num_dataset)\n",
    "    split2=int((train_size+(1-train_size)/2)*num_dataset)\n",
    "    control_random_seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, validation_idx, test_idx = indices[:split1], indices[split1:split2], indices[split2:]\n",
    "    train_image_path_list=[]\n",
    "    train_target_path_list=[]\n",
    "    validation_image_path_list=[]\n",
    "    validation_target_path_list=[]\n",
    "    test_image_path_list=[]\n",
    "    test_target_path_list=[]\n",
    "    for i, index in enumerate(indices):\n",
    "        if i<split1:\n",
    "            train_image_path_list.append(image_path_list[index])\n",
    "            train_target_path_list.append(target_path_list[index])\n",
    "        elif split1<=i and i<split2:\n",
    "            validation_image_path_list.append(image_path_list[index])\n",
    "            validation_target_path_list.append(target_path_list[index])\n",
    "        else:\n",
    "            test_image_path_list.append(image_path_list[index])\n",
    "            test_target_path_list.append(target_path_list[index])\n",
    "    train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, aug=True)\n",
    "    validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, aug=False)\n",
    "    test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, aug=False)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    num_workers=4, pin_memory=True, shuffle=True, \n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size, \n",
    "        num_workers=4, pin_memory=True,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=4, pin_memory=True,\n",
    "    )\n",
    "    print(f'{model_name} (Iter {iteration})')\n",
    "    output_dir = output_root + f'/{model_name}_Iter_{iteration}'\n",
    "    copy_sourcefile(output_dir, src_dir='src')\n",
    "    control_random_seed(seed)\n",
    "    model=str_to_class(model_name)(in_channels, number_of_classes)\n",
    "    device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "    if len(devices)>1:\n",
    "        model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "    df = contduct_experiments(seed, model_name, model, train_loader, validation_loader, test_loader,  optimizer, lr,  number_of_classes, epochs, Metrics, df, device,None)\n",
    "    df.to_csv(output_root+'/'+'Reflex_Seg_'+Experiments_Time+'.csv', index=False, header=True, encoding=\"cp949\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43989f70-2abc-42f4-88dc-3cd4a278b0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSH_Torch_2.0",
   "language": "python",
   "name": "lsh_torch_2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
